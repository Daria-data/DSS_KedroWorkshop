{
  "code": "def tune_hyperparameters(\n    feature_engineering: ColumnTransformer,\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    X_val: pd.DataFrame,\n    y_val: pd.Series,\n) -> dict[str, float]:\n    \"\"\"Search for the LightGBM hyper-parameters that maximise validation accuracy.\n\n    The search space replicates the ranges used during experimentation.\n    Optuna evaluates N_TRIALS random-seed-controlled trials and returns\n    the parameter set that gives the highest accuracy on the\n    validation split.\n\n    Args:\n        feature_engineering: Fully configured ColumnTransformer that\n            handles one-hot encoding and feature scaling.\n        X_train: Training features.\n        y_train: Training target.\n        X_val: Validation features.\n        y_val: Validation target.\n\n    Returns:\n        dict[str, float]: Mapping of parameter names to their optimal\n        values, ready to be passed into ``Pipeline.set_params``.\n    \"\"\"\n    N_TRIALS = 100\n    RANDOM_STATE = 42\n\n    def objective(trial: optuna.Trial) -> float:\n        params = {\n            # Meta-parameters (fixed choices)\n            \"model__objective\": trial.suggest_categorical(\n                \"model__objective\", [\"binary\"]\n            ),\n            \"model__metric\": trial.suggest_categorical(\n                \"model__metric\", [\"binary_logloss\"]\n            ),\n            \"model__boosting_type\": trial.suggest_categorical(\n                \"model__boosting_type\", [\"gbdt\"]\n            ),\n            \"model__verbosity\": trial.suggest_categorical(\"model__verbosity\", [-1]),\n            \"model__random_state\": trial.suggest_categorical(\n                \"model__random_state\", [RANDOM_STATE]\n            ),\n            # Hyper-parameters (to be tuned)\n            \"model__num_leaves\": trial.suggest_int(\"model__num_leaves\", 10, 100),\n            \"model__learning_rate\": trial.suggest_float(\n                \"model__learning_rate\", 0.01, 0.1, log=True\n            ),\n            \"model__feature_fraction\": trial.suggest_float(\n                \"model__feature_fraction\", 0.1, 1.0\n            ),\n            \"model__bagging_fraction\": trial.suggest_float(\n                \"model__bagging_fraction\", 0.1, 1.0\n            ),\n            \"model__bagging_freq\": trial.suggest_int(\"model__bagging_freq\", 1, 10),\n            \"model__min_child_samples\": trial.suggest_int(\n                \"model__min_child_samples\", 1, 50\n            ),\n        }\n\n        model_pipeline = Pipeline(\n            [\n                (\"feature_engineering\", feature_engineering),\n                (\"model\", LGBMClassifier()),\n            ]\n        ).set_params(**params)\n\n        model_pipeline.fit(X_train, y_train.values.ravel())\n        return model_pipeline.score(X_val, y_val.values.ravel())\n\n    optuna.logging.set_verbosity(optuna.logging.WARNING)\n    study = optuna.create_study(\n        direction=\"maximize\",\n        sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n    )\n    study.optimize(objective, n_trials=N_TRIALS)\n\n    return study.best_params\n",
  "filepath": "kedro-pro\\src\\kedro_pro\\pipelines\\model_training\\nodes.py",
  "parameters": {},
  "run_command": "kedro run --to-nodes='model_training.tune_hyperparameters'",
  "inputs": [
    "model_training.feature_engineering",
    "model_training.X_train",
    "model_training.y_train",
    "model_training.X_val",
    "model_training.y_val"
  ],
  "outputs": [
    "model_training.best_params"
  ]
}